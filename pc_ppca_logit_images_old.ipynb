{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad, hessian\n",
    "from autograd import elementwise_grad\n",
    "from scipy.optimize import fmin_l_bfgs_b, fmin_bfgs, fmin_cg, fmin_ncg\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import cho_factor, cho_solve, cholesky\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from autograd.misc.optimizers import adam\n",
    "import copy\n",
    "import glob\n",
    "import imageio\n",
    "import skimage\n",
    "from skimage import data, io, filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = np.array([skimage.img_as_float(imageio.imread(file)) for file in glob.glob('yale/*png')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr_rescaled = np.array([skimage.img_as_float(skimage.transform.rescale(imageio.imread(file),1.0 / 4.0)) for file in glob.glob('yale/*png')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 62, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr_rescaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(img_arr_rescaled[0])\n",
    "skimage.io.show()\n",
    "skimage.io.imshow(img_arr[0])\n",
    "skimage.io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 250, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape(img_arr, (img_arr.shape[0], img_arr.shape[1]*img_arr.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape(img_arr_rescaled, (img_arr_rescaled.shape[0], img_arr_rescaled.shape[1]*img_arr_rescaled.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(x.shape[0])\n",
    "for i in range(x.shape[0]):\n",
    "    if any(s in glob.glob('yale/*png')[i] for s in ('sad', 'wink', 'surprised', 'sleepy', 'happy')):\n",
    "        y[i] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 3100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def objective_logit (params, x, y, latent_dim, lambda_e, opt): #check the order of arguments!\n",
    "    N = x.shape[0]\n",
    "    D = x.shape[1]\n",
    "    f, bias_x, cov_noise, w = decode_parameters_fast(params, D, latent_dim, opt)\n",
    "    L = f.shape[1]\n",
    "        \n",
    "    icn = (1.0 / cov_noise).reshape((-1, 1))\n",
    "    xn = (x - bias_x).T\n",
    "    Ax = icn * xn\n",
    "    Au = icn * f\n",
    "    C = np.eye(L) + np.dot(f.T, Au)\n",
    "    temp1 = Ax - np.dot(Au, np.linalg.solve(C, np.dot(f.T, Ax)))\n",
    "    sign, log_det_cov_x = np.linalg.slogdet(C)\n",
    "    log_det_cov_x += np.sum(np.log(cov_noise))\n",
    "    \n",
    "    unnorm_log_pdf_x = np.einsum(\"nd,dn->n\", x - bias_x, temp1)\n",
    "    \n",
    "    mean_z = np.einsum(\"dl,dn->nl\", f, temp1)\n",
    "    temp2 = np.einsum(\"l,nl->n\", w[1:], mean_z) + w[0]\n",
    "    log_bern_pdf_y = np.log(sigmoid(np.multiply((2*y-1),temp2)))\n",
    "    log_prior = 0 \n",
    "    lambda_r = 1\n",
    "    #reg1 = lambda_r*np.sum(f**2)\n",
    "    #reg2 = lambda_r*np.sum(w**2)\n",
    "    #reg = reg1+reg2\n",
    "    reg = 0\n",
    "    ll = N*D*np.log(2*np.pi)/2 + log_det_cov_x*0.5*N + np.sum(unnorm_log_pdf_x)*0.5 \n",
    "    ll = ll - lambda_e*np.sum(log_bern_pdf_y) - log_prior + reg\n",
    "    return ll\n",
    "\n",
    "def decode_parameters(params, D, latent_dim, opt):\n",
    "    size_f = D*latent_dim\n",
    "    f =  params[:size_f]\n",
    "    #f = f*np.array([0,1]) + np.array([1,0])\n",
    "    f =  f.reshape(D, latent_dim)\n",
    "    bias_x = params[size_f:size_f+D]\n",
    "    if (opt==\"ppca\"):\n",
    "        var = params[size_f+D]\n",
    "        cov_noise= np.diag(np.ones(D)*np.log(1+np.exp(var)))\n",
    "        #cov_noise= np.diag(np.ones(D)*np.exp(var))\n",
    "        w = params[size_f+D+1:]\n",
    "    else:\n",
    "        var = params[size_f+D:size_f+D*2]\n",
    "        cov_noise= np.diag(np.log(1+np.exp(var)))\n",
    "        w = params[size_f+D*2:]\n",
    "    return f, bias_x, cov_noise, w\n",
    "\n",
    "def transform(f, bias, cov_noise, x):\n",
    "    cov_x = np.einsum(\"dl,ml->dm\",f,f) + cov_noise\n",
    "    temp = np.linalg.solve(cov_x, (x - bias).T)\n",
    "    mean_z = np.einsum(\"dl,dn->nl\", f, temp)\n",
    "    return mean_z\n",
    "\n",
    "def compute_ll(f,bias,cov_noise, x):\n",
    "    N = x.shape[0]\n",
    "    D = x.shape[1]\n",
    "    cov_x = np.einsum(\"dl,ml->dm\",f,f) + cov_noise\n",
    "    sign, log_det_cov_x = np.linalg.slogdet(cov_x)\n",
    "    temp1 = np.linalg.solve(cov_x, (x - bias).T)\n",
    "    ll = N*D*np.log(2*np.pi)/2\n",
    "    ll += log_det_cov_x*N/2\n",
    "    ll += np.sum(np.einsum(\"nd,dn->n\", x - bias, temp1))/2\n",
    "    return -ll\n",
    "\n",
    "def compute_pl_logit(f, bias_x, cov_noise, w, x, y):\n",
    "    cov_x = np.einsum(\"dl,ml->dm\",f,f) + cov_noise\n",
    "    temp1 = np.linalg.solve(cov_x, (x - bias_x).T)\n",
    "    mean_z = np.einsum(\"dl,dn->nl\", f, temp1)\n",
    "    temp2 = np.einsum(\"l,nl->n\", w[1:], mean_z) + w[0]\n",
    "    log_bern_pdf_y = np.log(sigmoid(np.multiply((2*y-1),temp2)))\n",
    "    return np.sum(log_bern_pdf_y)\n",
    "\n",
    "def decode_parameters_fast(params, D, latent_dim, opt):\n",
    "    size_f = D*latent_dim\n",
    "    f =  params[:size_f]\n",
    "    #f = f*np.array([0,1]) + np.array([1,0])\n",
    "    f =  f.reshape(D, latent_dim)\n",
    "    bias_x = params[size_f:size_f+D]\n",
    "    if (opt==\"ppca\"):\n",
    "        var = params[size_f+D]\n",
    "        cov_noise= np.ones(D)*np.log(1+np.exp(var))\n",
    "        #cov_noise= np.diag(np.ones(D)*np.exp(var))\n",
    "        w = params[size_f+D+1:]\n",
    "    else:\n",
    "        var = params[size_f+D:size_f+D*2]\n",
    "        cov_noise = np.log(1+np.exp(var))\n",
    "        w = params[size_f+D*2:]\n",
    "    return f, bias_x, cov_noise, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "x_dim = x.shape[1]      \n",
    "size_f = latent_dim*x_dim\n",
    "\n",
    "params_size_logit_fa = x_dim*latent_dim + x_dim + x_dim + latent_dim + 1\n",
    "params_size_norm_fa = x_dim*latent_dim + x_dim + x_dim + latent_dim\n",
    "params_size_logit_ppca = x_dim*latent_dim + x_dim + 1 + latent_dim + 1\n",
    "params_size_norm_ppca = x_dim*latent_dim + x_dim + 1 + latent_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = \"ppca\"\n",
    "if (opt == \"fa\"):\n",
    "    params_size = params_size_logit_fa\n",
    "else:\n",
    "    params_size = params_size_logit_ppca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "transformer = PCA(latent_dim)\n",
    "x_proj_pca = transformer.fit_transform(x)\n",
    "clf = LogisticRegression().fit(x_proj_pca, y)\n",
    "acc_score_pca = clf.score(x_proj_pca, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_fa = FactorAnalysis(latent_dim)\n",
    "x_proj_fa = transformer.fit_transform(x)\n",
    "clf = LogisticRegression().fit(x_proj_fa, y)\n",
    "acc_score_fa = clf.score(x_proj_fa, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FactorAnalysis' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9af7822c6cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer_fa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'FactorAnalysis' object has no attribute 'components_'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(x, y)\n",
    "acc_score_orig = clf.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sklearn = transformer.components_.T\n",
    "cov_noise_sklearn = np.diag(np.ones(x_dim)*transformer.noise_variance_)\n",
    "bias_sklearn = np.mean(x, axis = 0)\n",
    "print \"F sklearn\", f_sklearn \n",
    "print \"mean of x \\n\", bias_sklearn \n",
    "print \"cov noise sklearn\\n\", cov_noise_sklearn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ll(f_sklearn, bias_sklearn, cov_noise_sklearn, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters\n",
      "F: [[0.86361742 0.2355222  0.88850065 ... 0.67123869 0.88760575 0.21885433]\n",
      " [0.50260063 0.27849017 0.43636393 ... 0.09593587 0.42263038 0.61544172]\n",
      " [0.46564178 0.53018685 0.80685484 ... 0.02530548 0.37167352 0.09453595]\n",
      " ...\n",
      " [0.33170219 0.29020874 0.01553651 ... 0.26673531 0.10267912 0.16523587]\n",
      " [0.65992233 0.25550432 0.01537221 ... 0.34414316 0.48902544 0.33284171]\n",
      " [0.40025097 0.25679528 0.11733277 ... 0.48657463 0.40348414 0.33505653]]\n",
      "slope [0.5819714  1.18243702 0.49112393 0.85361542 0.27801165 6.01039355\n",
      " 8.18134554 0.14292364 0.47614651 2.8121066 ]\n",
      "bias:  [0.69522956 0.1238199  0.76589619 ... 0.25150356 0.72394408 0.5949079 ]\n",
      "cov_noise matrix:  [[0.97335665 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.97335665 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.97335665 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.97335665 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.97335665 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.97335665]]\n",
      "bias for y:\n",
      "0.20516268067839216\n",
      "logistic regression weights:\n",
      "[0.58081372 0.80145758 0.66160201 0.36864326 0.93581797 0.83695128\n",
      " 0.12104372 0.585834   0.3929237  0.02461366]\n",
      "initial obj value logit 423904.4076945441\n"
     ]
    }
   ],
   "source": [
    "init_params = np.random.rand(params_size)\n",
    "f, bias, cov_noise, w =  decode_parameters(init_params, x_dim, latent_dim, opt)\n",
    "print \"Initial parameters\"\n",
    "print \"F:\", f\n",
    "print \"slope\", f[1]/f[0]\n",
    "print \"bias: \", bias\n",
    "print \"cov_noise matrix: \", cov_noise\n",
    "print \"bias for y:\\n\", w[0]\n",
    "print \"logistic regression weights:\\n\", w[1:]\n",
    "print \"initial obj value logit\", objective_logit(init_params, x, y, latent_dim, 0, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_objective_logit = grad(objective_logit, argnum = 0)\n",
    "lambda_e = 100000\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "params_optimizied_logit, obj, dict = fmin_l_bfgs_b(objective_logit, x0 = init_params, fprime = grad_objective_logit, args = (x, y, latent_dim, lambda_e, opt))#, callback=callback)\n",
    "t1 = time.time()\n",
    "print t1-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_logit, bias_logit, cov_noise_logit, w_logit = decode_parameters(params_optimizied_logit, x_dim, latent_dim, opt)\n",
    "x_proj_logit = transform(f_logit, bias_logit, cov_noise_logit, x)\n",
    "clf_pc = LogisticRegression().fit(x_proj_logit, y)\n",
    "acc_scores = clf_pc.score(x_proj_logit, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_e_1 = 0.4\n",
    "lambda_e_2 = 0.51\n",
    "step  = 0.01\n",
    "lambda_es = np.arange(lambda_e_1,lambda_e_2,step)\n",
    "\n",
    "acc_scores = np.zeros(lambda_es.shape[0])\n",
    "obj_values_logit = np.zeros(lambda_es.shape[0])\n",
    "ll_logit = np.zeros(lambda_es.shape[0])\n",
    "pred_l_logit = np.zeros(lambda_es.shape[0])\n",
    "\n",
    "\n",
    "def callback(params):\n",
    "    print objective_logit_1(params, transformer.noise_variance_, x, y, latent_dim, lambda_es[i], opt)\n",
    "    print grad_objective_logit_1(params, transformer.noise_variance_, x, y, latent_dim, lambda_es[i], opt)\n",
    "    #print params[1]/params[0]\n",
    "    #print params[-2],\"-\",params[-1]\n",
    "    pass\n",
    "    print '\\n'\n",
    "grad_objective_logit = grad(objective_logit, argnum = 0)\n",
    "\n",
    "for i in range(lambda_es.shape[0]):\n",
    "    params_optimizied_logit, obj, dict = fmin_l_bfgs_b(objective_logit, x0 = init_params, fprime = grad_objective_logit, args = (x, y, latent_dim, lambda_es[i], opt))#, callback=callback)\n",
    "    f_logit, bias_logit, cov_noise_logit, w_logit = decode_parameters(params_optimizied_logit, x_dim, latent_dim, opt)\n",
    "    #cov_noise_logit = np.diag(np.ones(x_dim)*transformer.noise_variance_)\n",
    "    x_proj_logit = transform(f_logit, bias_logit, cov_noise_logit, x)\n",
    "    clf_pc = LogisticRegression().fit(x_proj_logit, y)\n",
    "    acc_scores[i] = clf_pc.score(x_proj_logit, y)\n",
    "    ll_logit[i] = compute_ll(f_logit, bias_logit, cov_noise_logit, x)\n",
    "    obj_values_logit[i]=  obj\n",
    "    pred_l_logit[i] =  compute_pl_logit(f_logit, bias_logit, cov_noise_logit, w_logit, x, y) \n",
    "    \n",
    "acc_scores_adam = np.zeros(lambda_es.shape[0])\n",
    "obj_values_logit_adam = np.zeros(lambda_es.shape[0])\n",
    "ll_logit_adam = np.zeros(lambda_es.shape[0])\n",
    "pred_l_logit_adam = np.zeros(lambda_es.shape[0])\n",
    "\n",
    "for i in range(lambda_es.shape[0]):\n",
    "    print i\n",
    "    params_opt_adam_logit = adam(get_grad_obj(init_params, x, y, latent_dim, lambda_es[i], opt, iter), init_params, step_size = step_size,  num_iters=num_iters)\n",
    "    \n",
    "    f_logit, bias_logit, cov_noise_logit, w_logit = decode_parameters(params_opt_adam_logit, x_dim, latent_dim, opt)\n",
    "    x_proj_logit = transform(f_logit, bias_logit, cov_noise_logit, x)\n",
    "    \n",
    "    clf_pc = LogisticRegression().fit(x_proj_logit, y)\n",
    "    acc_scores_adam[i] = clf_pc.score(x_proj_logit, y)\n",
    "    \n",
    "    ll_logit_adam[i] = compute_ll(f_logit, bias_logit, cov_noise_logit, x)\n",
    "    obj_values_logit_adam[i]=  objective_logit(params_opt_adam_logit, x, y, latent_dim, lambda_es[i], opt)\n",
    "    pred_l_logit_adam[i] =  compute_pl_logit(f_logit, bias_logit, cov_noise_logit, w_logit, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_parameters(params_opt_adam_logit, x_dim, latent_dim, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,20))\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.title(\"Final obj values \")\n",
    "plt.xticks(np.arange(lambda_e_1,lambda_e_2, step))\n",
    "plt.plot(lambda_es, obj_values_logit,  label = \"l_bfgs_b\")\n",
    "plt.plot(lambda_es, obj_values_logit_adam,  label = \"adam\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.title(\"Final log-likelihood\")\n",
    "plt.xticks(np.arange(lambda_e_1,lambda_e_2, step))\n",
    "plt.plot(lambda_es, ll_logit,  label = \"l_bfgs_b\")\n",
    "plt.plot(lambda_es, ll_logit_adam,  label = \"adam\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.title(\"Train accuracy\")\n",
    "plt.xticks(np.arange(lambda_e_1,lambda_e_2, step))\n",
    "plt.plot(lambda_es, acc_scores,  label = \"l_bfgs_b\")\n",
    "plt.plot(lambda_es, acc_scores_adam,  label = \"adam\", )\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.title(\"Prediction log-likelihood\")\n",
    "plt.xticks(np.arange(lambda_e_1,lambda_e_2, step))\n",
    "plt.plot(lambda_es, pred_l_logit, label = \"l_bfgs_b\")\n",
    "plt.plot(lambda_es, pred_l_logit_adam,  label = \"adam\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"lambda_e\")\n",
    "\n",
    "plt.savefig(\"adam_vs_l_bfgs_b_lambda_e{},{}.png\".format(lambda_e_1, lambda_e_2))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#params_optimizied_logit_1, obj_value_logit, dict = fmin_l_bfgs_b(objective_logit_1, x0 = params, fprime = grad_objective_logit_1, args = (transformer.noise_variance_, x, y, latent_dim, lambda_e, opt))\n",
    "\n",
    "params_optimizied_logit_1, obj_value_logit_1, dict = fmin_l_bfgs_b(objective_logit, x0 = params, fprime = grad_objective_logit, args = (x, y, latent_dim, lambda_e_1, opt))\n",
    "params_optimizied_logit_2, obj_value_logit_2, dict = fmin_l_bfgs_b(objective_logit, x0 = params, fprime = grad_objective_logit, args = (x, y, latent_dim, lambda_e_2, opt))\n",
    "\n",
    "\n",
    "f_logit_1, bias_logit_1, cov_noise_logit_1, w_logit_1 = decode_parameters(params_optimizied_logit_1, x_dim, latent_dim, opt)\n",
    "ll_1=  compute_ll(f_logit_1, bias_logit_1, cov_noise_logit_1, x)\n",
    "pl_1 = compute_pl_logit(f_logit_1, bias_logit_1, cov_noise_logit_1, w_logit_1, x, y) \n",
    "\n",
    "f_logit_2, bias_logit_2, cov_noise_logit_2, w_logit_2 = decode_parameters(params_optimizied_logit_2, x_dim, latent_dim, opt)\n",
    "ll_2=  compute_ll(f_logit_2, bias_logit_2, cov_noise_logit_2, x)\n",
    "pl_2 = compute_pl_logit(f_logit_2, bias_logit_2, cov_noise_logit_2, w_logit_2, x, y) \n",
    "\n",
    "#lambda_es = np.arange(0,1,0.1)\n",
    "lambda_es = np.array([lambda_e_1, lambda_e_2])\n",
    "obj_values_logit = np.ones((2, lambda_es.shape[0]))\n",
    "\n",
    "for i in range(lambda_es.shape[0]):\n",
    "    obj_values_logit[0,i] = objective_logit(params_optimizied_logit_1, x, y, latent_dim, lambda_es[i], opt)\n",
    "    obj_values_logit[1,i] = objective_logit(params_optimizied_logit_2, x, y, latent_dim, lambda_es[i], opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = np.arange(0, 1.0, 0.01)\n",
    "obj1 = [objective_logit(params_optimizied_logit_1 * i + (1.0-i) * params_optimizied_logit_2, x, y, latent_dim, lambda_e_1, opt) for i in np.arange(0, 1, 0.01)]\n",
    "obj2 = [objective_logit(params_optimizied_logit_1 * i + (1.0-i) * params_optimizied_logit_2, x, y, latent_dim, lambda_e_2, opt) for i in np.arange(0, 1, 0.01)]\n",
    "\n",
    "plt.plot(interp, obj1)\n",
    "plt.plot(interp, obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = np.arange(-0., 1.0, 0.01)\n",
    "lik =[]\n",
    "plik = []\n",
    "obj = []\n",
    "grad_objective_logit_1 = grad(objective_logit_1)\n",
    "params_start= np.random.rand(params_size)\n",
    "for i in interp:\n",
    "    slope = 3.83374105 * i + (1.0 - i) * -7.39114372\n",
    "    f_logit = np.array([1., slope]).reshape((2,1))\n",
    "    print f_logit\n",
    "    params_optimizied_logit, obj_value_logit, dict = fmin_l_bfgs_b(objective_logit_1, x0 = params_start, fprime = grad_objective_logit_1, args = (f_logit, x, y, latent_dim, lambda_e_1, opt))\n",
    "    _, bias_logit, cov_noise_logit, w_logit = decode_parameters(params_optimizied_logit, x_dim, latent_dim, opt)\n",
    "    ll =  compute_ll(f_logit, bias_logit, cov_noise_logit, x)\n",
    "    pl = compute_pl_logit(f_logit, bias_logit, cov_noise_logit, w_logit, x, y) \n",
    "    lik.append(ll)\n",
    "    plik.append(pl)\n",
    "    obj.append(obj_value_logit)\n",
    "    \n",
    "plt.plot(interp, lik, label='lik')\n",
    "#plt.plot(interp, plik, label='predictive')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(interp, plik, label='predictive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(interp, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj[0], obj[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_values_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_logit_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_logit_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,7))\n",
    "plt.title(\"Final obj values \")\n",
    "plt.xticks(lambda_es)\n",
    "plt.plot(lambda_es, obj_values_logit[0, :], label = \"lambda_e_1 = {}\".format(lambda_e_1))\n",
    "plt.plot(lambda_es, obj_values_logit[1, :], label = \"lambda_e_2 = {}\".format(lambda_e_2))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221) \n",
    "#plt.axis('equal')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title(\"sklearn PCA projection\")\n",
    "x_proj_sklearn = transform(f_sklearn, bias_sklearn, cov_noise_sklearn, x)\n",
    "\n",
    "plt.plot(x_proj_sklearn[:n_samples_class,0], np.ones(n_samples_class), 'x')\n",
    "plt.plot(x_proj_sklearn[n_samples_class:,0], np.zeros(n_samples_class), 'o')\n",
    "\n",
    "\n",
    "plt.subplot(222) \n",
    "plt.title(\"PC PPCA projection: lambda_e = {}\".format(lambda_e_1))\n",
    "\n",
    "x_proj_logit_1 = transform(f_logit_1, bias_logit_1, cov_noise_logit_1, x)\n",
    "plt.plot(x_proj_logit_1[:n_samples_class], np.ones(n_samples_class), 'x')\n",
    "plt.plot(x_proj_logit_1[n_samples_class:], np.zeros(n_samples_class), 'o')\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title(\"PC PPCA projection: lambda_e = {}\".format(lambda_e_2))\n",
    "x_proj_logit_2 = transform(f_logit_2, bias_logit_2, cov_noise_logit_2, x)\n",
    "plt.plot(x_proj_logit_2[:n_samples_class], np.ones(n_samples_class), 'x')\n",
    "plt.plot(x_proj_logit_2[n_samples_class:], np.zeros(n_samples_class), 'o')\n",
    "\n",
    "\n",
    "plt.subplot(224) \n",
    "plt.axis('equal')\n",
    "clf_pc_1 = LogisticRegression().fit(x_proj_logit_1, y)\n",
    "acc_scores_1 = clf_pc.score(x_proj_logit_1, y)\n",
    "clf_pc_2 = LogisticRegression().fit(x_proj_logit_2, y)\n",
    "acc_scores_2 = clf_pc.score(x_proj_logit_2, y)\n",
    "\n",
    "    \n",
    "plt.plot(x[:n_samples_class,0], x[:n_samples_class,1], 'x')\n",
    "plt.plot(x[n_samples_class:,0], x[n_samples_class:,1], 'o')\n",
    "\n",
    "x_lim_1 = -5\n",
    "x_lim_2 = 7\n",
    "\n",
    "\n",
    "a =  np.array(range(x_lim_1, x_lim_2)) \n",
    "\n",
    "b_pc_fa_logit_1 = f_logit_1[1]/f_logit_1[0]*(a + bias_logit_1[0]) + bias_logit_1[1]\n",
    "b_pc_fa_logit_2 = f_logit_2[1]/f_logit_2[0]*(a + bias_logit_2[0]) + bias_logit_2[1]\n",
    "b_sklearn = f_sklearn[1]/f_sklearn[0]*(a + bias_sklearn [0]) + bias_sklearn [1]\n",
    "\n",
    "plt.plot(a, b_sklearn, label='sklearn pca')\n",
    "plt.plot(a, b_pc_fa_logit_1, label=\"pc ppca lambda_e = {}\".format(lambda_e_1))\n",
    "plt.plot(a, b_pc_fa_logit_2, label=\"pc ppca lambda_e = {}\".format(lambda_e_2))\n",
    "plt.ylim(x_lim_1,x_lim_2)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"data_lambda_e_1={}_lambda_e_2={}_logistic_pc_ppca_vs_sklearn_pca.png\".format(lambda_e_1, lambda_e_2))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221) \n",
    "#plt.axis('equal')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title(\"sklearn PCA projection\")\n",
    "x_proj_sklearn = transform(f_sklearn, bias_sklearn, cov_noise_sklearn, x)\n",
    "\n",
    "plt.plot(x_proj_sklearn[:n_samples_class,0], np.ones(n_samples_class), 'x')\n",
    "plt.plot(x_proj_sklearn[n_samples_class:,0], np.zeros(n_samples_class), 'o')\n",
    "\n",
    "\n",
    "plt.subplot(222) \n",
    "\n",
    "plt.title(\"PC PPCA projection opt fmin_l_bfgs_b\")\n",
    "\n",
    "f_logit_1, bias_logit_1, cov_noise_logit_1, w_logit_1 = decode_parameters(params_optimizied_logit, x_dim, latent_dim, opt)\n",
    "x_proj_logit_1 = transform(f_logit_1, bias_logit_1, cov_noise_logit_1, x)\n",
    "plt.plot(x_proj_logit_1[:n_samples_class], np.ones(n_samples_class), 'x')\n",
    "plt.plot(x_proj_logit_1[n_samples_class:], np.zeros(n_samples_class), 'o')\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "plt.title(\"PC PPCA projection opt ADAM\")\n",
    "\n",
    "f_logit_2, bias_logit_2, cov_noise_logit_2, w_logit_2 = decode_parameters(params_opt_adam_logit, x_dim, latent_dim, opt)\n",
    "x_proj_logit_2 = transform(f_logit_2, bias_logit_2, cov_noise_logit_2, x)\n",
    "plt.plot(x_proj_logit_2[:n_samples_class], np.ones(n_samples_class), 'x')\n",
    "plt.plot(x_proj_logit_2[n_samples_class:], np.zeros(n_samples_class), 'o')\n",
    "\n",
    "\n",
    "plt.subplot(224) \n",
    "plt.axis('equal')\n",
    "clf_pc_1 = LogisticRegression().fit(x_proj_logit_1, y)\n",
    "acc_scores_1 = clf_pc.score(x_proj_logit_1, y)\n",
    "clf_pc_2 = LogisticRegression().fit(x_proj_logit_2, y)\n",
    "acc_scores_2 = clf_pc.score(x_proj_logit_2, y)\n",
    "\n",
    "plt.title(\"train acc at lambda_e = {} is {} for fmin_l_bfgs_b, and {} for ADAM\".format(lambda_es[i], acc_scores[i], acc_scores_adam[i]))\n",
    "\n",
    "\n",
    "plt.plot(x[:n_samples_class,0], x[:n_samples_class,1], 'x')\n",
    "plt.plot(x[n_samples_class:,0], x[n_samples_class:,1], 'o')\n",
    "\n",
    "x_lim_1 = -5\n",
    "x_lim_2 = 7\n",
    "\n",
    "\n",
    "a =  np.array(range(x_lim_1, x_lim_2)) \n",
    "\n",
    "b_pc_fa_logit_1 = f_logit_1[1]/f_logit_1[0]*(a + bias_logit_1[0]) + bias_logit_1[1]\n",
    "b_pc_fa_logit_2 = f_logit_2[1]/f_logit_2[0]*(a + bias_logit_2[0]) + bias_logit_2[1]\n",
    "b_sklearn = f_sklearn[1]/f_sklearn[0]*(a + bias_sklearn [0]) + bias_sklearn [1]\n",
    "\n",
    "plt.plot(a, b_sklearn, label='sklearn pca')\n",
    "plt.plot(a, b_pc_fa_logit_1, label=\"fmin_l_bfgs_b\")\n",
    "plt.plot(a, b_pc_fa_logit_2, label=\"adam\")\n",
    "plt.ylim(x_lim_1,x_lim_2)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"data_lambda_e_=_{}_logistic_pc_ppca_adam_vs_l_bfgs_b_vs_sklearn_pca.png\".format(lambda_es[i]))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pc_fa_logit_1[(b_pc_fa_logit_1<np.max(x[:,1] and b_pc_fa_logit_1>np.min(x[:,1]))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pc_fa_logit_1[b_pc_fa_logit_1[b_pc_fa_logit_1>np.min(x[:,1])]< np.max(x[:,1])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ll_1\n",
    "print pl_1\n",
    "print ll_2\n",
    "\n",
    "print pl_2\n",
    "\n",
    "print \"Final parameters_logit 1\"\n",
    "print \"logit slope\\n\", f_logit_1[1]/f_logit_1[0]\n",
    "print \"F:\\n\", f_logit_1\n",
    "print \"bias for x:\\n\", bias_logit_1\n",
    "print \"var :\\n\",cov_noise_logit_1\n",
    "print \"bias for y:\\n\", w_logit_1[0]\n",
    "print \"logistic regression weights:\\n\", w_logit_1[1:]\n",
    "\n",
    "print \"Final parameters_logit 2\"\n",
    "print \"logit slope\\n\", f_logit_2[1]/f_logit_2[0]\n",
    "print \"F:\\n\", f_logit_2\n",
    "print \"bias for x:\\n\", bias_logit_2\n",
    "print \"cov_noise matrix:\\n\", cov_noise_logit_2\n",
    "print \"bias for y:\\n\", w_logit_2[0]\n",
    "print \"logistic regression weights:\\n\", w_logit_2[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[ 1.        ], [-7.39114372]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum(\"dl,ml->dm\",t,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum(\"dl,ml->dm\",f_logit_2,f_logit_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
